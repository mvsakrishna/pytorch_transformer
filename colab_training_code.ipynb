{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mCctkv3faXlz",
        "outputId": "c4f31cbc-03ed-42b4-cd5f-7d0472437a9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mvsakrishna/pytorch_transformer.git\n",
        "%cd pytorch_transformer"
      ],
      "metadata": {
        "id": "Wib1zyx9LHuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64220c2d-71ad-4886-e132-0b7b0a207e24"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch_transformer'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 13 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (13/13), 1.10 MiB | 37.50 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "/content/pytorch_transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-qC9wRGTbSy",
        "outputId": "0fbac42b-3421-4d3a-f9e9-1e3b52d9882d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/510.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m440.3/510.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "id": "s_RLJtKaNRfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f5c5aa-b02b-46ef-b3dd-626d5fb3f13a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-15 01:42:16.228990: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-15 01:42:16.229052: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-15 01:42:16.231044: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-15 01:42:17.460535: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Using device: cuda\n",
            "Device name: NVIDIA A100-SXM4-40GB\n",
            "Device memory: 39.56427001953125 GB\n",
            "Downloading readme: 100% 28.1k/28.1k [00:00<00:00, 52.3MB/s]\n",
            "Downloading data: 100% 5.73M/5.73M [00:00<00:00, 8.68MB/s]\n",
            "Generating train split: 100% 32332/32332 [00:00<00:00, 399248.19 examples/s]\n",
            "\u001b[2K[00:00:01] Pre-processing sequences       ██████████████████ 0        /        0Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n",
            "No model to preload, starting from scratch\n",
            "Processing Epoch 00: 100% 3638/3638 [06:33<00:00,  9.25it/s, loss=5.542]\n",
            "\n",
            "    SOURCE: \"I _can_ do what he wants me to do: I am forced to see and acknowledge that,\" I meditated,--\"that is, if life be spared me.\n",
            "    TARGET: — Posso fare quello che mi chiede, — dissi a me stessa, — debbo riconoscerlo.\n",
            " PREDICTED: — Non non ho detto che non ho potuto , e che non , e che non .\n",
            "\n",
            "    SOURCE: Well, Kitty, have you been skating again?'\n",
            "    TARGET: Ebbene, Kitty, hai pattinato di nuovo?\n",
            " PREDICTED: Ma non è di questo ?\n",
            "\n",
            "Processing Epoch 01: 100% 3638/3638 [06:32<00:00,  9.28it/s, loss=5.960]\n",
            "\n",
            "    SOURCE: I summoned strength to ask what had caused this calamity.\n",
            "    TARGET: Radunai dunque le forze per domandare che cosa aveva cagionato quella disgrazia.\n",
            " PREDICTED: Io mi , ma io non mi .\n",
            "\n",
            "    SOURCE: \"Dead?\" repeated Diana.\n",
            "    TARGET: — Morto? — ripetè Diana.\n",
            " PREDICTED: — ? — esclamò .\n",
            "\n",
            "Processing Epoch 02: 100% 3638/3638 [06:31<00:00,  9.29it/s, loss=5.308]\n",
            "\n",
            "    SOURCE: 'I know, I know,' he said with a smile. 'I am a family man myself.\n",
            "    TARGET: — Lo so, lo so — disse il dottore, sorridendo — io stesso ho famiglia; ma noi mariti, in questi momenti, siamo le persone più pietose.\n",
            " PREDICTED: — Io ho detto — disse , — io mi sono molto contento di me .\n",
            "\n",
            "    SOURCE: \"Are there ladies at the Leas?\"\n",
            "    TARGET: — Vi sono delle signore in quella villa?\n",
            " PREDICTED: — È una donna di luna ?\n",
            "\n",
            "Processing Epoch 03: 100% 3638/3638 [06:31<00:00,  9.29it/s, loss=3.559]\n",
            "\n",
            "    SOURCE: \"I'll stay with you, _dear_ Helen: no one shall take me away.\"\n",
            "    TARGET: — Rimarrò, cara Elena, e nessuno potrà strapparmi di qui.\n",
            " PREDICTED: — Mi pare che vi , e mi .\n",
            "\n",
            "    SOURCE: 'Only mustard isn't a bird,' Alice remarked.\n",
            "    TARGET: — Ma la mostarda non è un uccello, — osservò Alice.\n",
            " PREDICTED: — Non è un po ' di vino , — disse Alice .\n",
            "\n",
            "Processing Epoch 04: 100% 3638/3638 [06:31<00:00,  9.29it/s, loss=4.763]\n",
            "\n",
            "    SOURCE: As this horse approached, and as I watched for it to appear through the dusk, I remembered certain of Bessie's tales, wherein figured a North-of-England spirit called a \"Gytrash,\" which, in the form of horse, mule, or large dog, haunted solitary ways, and sometimes came upon belated travellers, as this horse was now coming upon me.\n",
            "    TARGET: Questo spirito, che appariva ora sotto la forma di cavallo, di mulo o di grosso cane, frequentava le vie solitarie e si mostrava ai viaggiatori in ritardo. Il cavallo era vicinissimo, quando allo scalpitio sentii aggiungersi un altro rumore che usciva dalla siepe, e vidi passare lungo i nocciuoli un cagnone, che, per il pelame bianco e nero, non poteva esser confuso con gli alberi.\n",
            " PREDICTED: Mentre il cavallo si , mi come un po ' di testa , mi , a un di vita , di cui mi , la testa , la quale mi , la testa , mi , mi , mi , mi , mi la testa , mi , mi la testa , mi in cui mi la testa , mi .\n",
            "\n",
            "    SOURCE: A ridge of lighted heath, alive, glancing, devouring, would have been a meet emblem of my mind when I accused and menaced Mrs. Reed: the same ridge, black and blasted after the flames are dead, would have represented as meetly my subsequent condition, when half-an-hour's silence and reflection had shown me the madness of my conduct, and the dreariness of my hated and hating position.\n",
            "    TARGET: Quando avevo accusato e minacciato la signora Reed, la mia anima era in fiamme, ma dopo una mezz'ora di silenzio e di riflessione riconobbi la pazzia commessa e la tristezza della mia posizione di bimba che odia e che è odiata.\n",
            " PREDICTED: la , la , la signora Fairfax era stata una signora , e la signora Reed era stata , e la mia , la mia , la mia vita è stata la mia , la mia vita , e la mia vita era stata la mia vita .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install translate"
      ],
      "metadata": {
        "id": "dpR5BI65WSuE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7cee0bb-b021-42e5-e832-ed5d1fa0dc2c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting translate\n",
            "  Downloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from translate) (8.1.7)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from translate) (4.9.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from translate) (2.31.0)\n",
            "Collecting libretranslatepy==2.1.1 (from translate)\n",
            "  Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2024.2.2)\n",
            "Installing collected packages: libretranslatepy, translate\n",
            "Successfully installed libretranslatepy-2.1.1 translate-3.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from config import get_config, latest_weights_file_path\n",
        "from train import get_model, get_ds, run_validation\n",
        "from translate import translate"
      ],
      "metadata": {
        "id": "f-dLTs9zRw3a"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "config = get_config()\n",
        "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)"
      ],
      "metadata": {
        "id": "RsALbuFgWCCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585cbe33-f53b-40ac-bf7f-fe30099caffc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained weights\n",
        "model_filename = latest_weights_file_path(config)\n",
        "state = torch.load(model_filename)\n",
        "model.load_state_dict(state['model_state_dict'])"
      ],
      "metadata": {
        "id": "VWRao-3sWENt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d58d8b2f-e7a1-4e0c-af3e-e00ebbf1e405"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: print(msg), 0, None, num_examples=10)"
      ],
      "metadata": {
        "id": "0o5a_ETCWKu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd27d4a8-739a-445f-8164-38c1be258f52"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: What will you tell him if you yourself know nothing?\n",
            "    TARGET: Cosa gli direte se non saprete nulla?\n",
            " PREDICTED: Che cosa volete dire , se non volete dire ?\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: No ill-usage so brands its record on my feelings.\n",
            "    TARGET: Nessun maltrattamento ha lasciato in me traccia così profonda.\n",
            " PREDICTED: No , , la mia vita .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The first thing she heard was a general chorus of 'There goes Bill!' then the Rabbit's voice along--'Catch him, you by the hedge!' then silence, and then another confusion of voices--'Hold up his head--Brandy now--Don't choke him--How was it, old fellow?\n",
            "    TARGET: La prima cosa che sentì fu un coro di voci che diceva: — Ecco Guglielmo che vola! — e poi la voce sola del Coniglio: — Pigliatelo voi altri presso la siepe! — e poi silenzio, e poi di nuovo una gran confusione di voci... — Sostenetegli il capo... un po' d'acquavite... Non lo strozzate...\n",
            " PREDICTED: La prima cosa che aveva sentito una bella ! — gridò il Coniglio bianco , — e poi , quando la Regina , quando la testa è stata una , e la testa , la testa , non è mai stata una , e non c ’ è mai una , non è mai una , né l ' altro ?\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: After working all through the spring and part of the summer, it was not till July that he prepared to go to his brother's in the country.\n",
            "    TARGET: Dopo aver lavorato tutta la primavera e parte dell’estate, soltanto nel mese di luglio era pronto per andare in campagna dal fratello.\n",
            " PREDICTED: Dopo aver fatto il frumento , e il frumento non era stato già già già già da pranzo , si era già andato a casa del fratello .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Mother!\" she exclaimed, \"there is a woman wants me to give her these porridge.\"\n",
            "    TARGET: — Mamma! — esclamò; — c'è una donna che mi chiede la minestra.\n",
            " PREDICTED: — Mamma ! — esclamò lei , — mi una donna .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Good morning, Grace,\" I said.\n",
            "    TARGET: — Buon giorno, Grace, — le dissi.\n",
            " PREDICTED: — Buona sera , signora ! — dissi .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: And I am sorry I came!'\n",
            "    TARGET: E mi spiace molto d’esser venuto.\n",
            " PREDICTED: E io sono felice !\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: One of those disputes, in which Katavasov evidently thought he had been the victor, was the first thing Levin remembered when he recognized him.\n",
            "    TARGET: E una di quelle conversazioni, in cui Katavasov, evidentemente, pensava d’aver avuto il sopravvento, fu la prima cosa che ricordò Levin, dopo averlo riconosciuto.\n",
            " PREDICTED: Un ’ altra parte , che , evidentemente , aveva capito , aveva capito che , quando Levin aveva detto che Levin aveva capito che Levin gli era stato stato detto .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Abundance of such things as these assisted to argue me out of all apprehensions of its being the devil; and I presently concluded then that it must be some more dangerous creature—viz. that it must be some of the savages of the mainland opposite who had wandered out to sea in their canoes, and either driven by the currents or by contrary winds, had made the island, and had been on shore, but were gone away again to sea; being as loath, perhaps, to have stayed in this desolate island as I would have been to have had them.\n",
            "    TARGET: Mi accorsero in copia altre considerazioni simili a queste ed atte a liberarmi affatto dalla paura che in ciò avesse parte il demonio. Dovetti quindi prestamente conchiuderne che la cosa dovesse attribuirsi a qualche creatura anche più pericolosa: vale a dire, bisognava credere che alcuni selvaggi abitanti del continente postomi di rimpetto, tratti fuor di via nelle loro piroghe, o pure spinti da correnti e venti contrari, avessero approdato nell’isola; indi si fossero imbarcati di nuovo, avendo forse a schifo il soggiorno di questo deserto, come da vero lo avrei avuto io se fossi stato ne’ loro panni.\n",
            " PREDICTED: di queste cose , come mi a tal caso , mi in questo caso , e a la spiaggia per la spiaggia , perchè non poteva essere più più più di più di più che di più di più , perchè la spiaggia non poteva essere più grande , nè per di più di più di più , nè di più che di più di più di più di più di più di più di più di più di più , nè di più che di più di più di più di più di più di più che di più di più di più di più , nè di più che di più , nè di più che di più che di più di più di più di più di più di più che di più di più di più di più di più di più di più di più di più di più di più di più di più , nè di più di più di più di più di più di più di più , nè di più che di più di più di più di più di più di più di più di più di più , nè di più di più di più di più di più che di più che di più di più di più di più che di più che di più che di più che di più che di più che di più che di più che di più che di più che di più che di più che di più che di più che di più che di mare di più che di più che di più che di più che di più che di più che di più che di più che di cui la spiaggia di più che di più che la spiaggia , se non essere , se non essere , se non essere più che di più che di più che la spiaggia di più che la spiaggia di più che la spiaggia , se non\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: 'Yes, but don't forget what you are and what I am...\n",
            "    TARGET: — Sì, ma tu non devi dimenticare cosa sei tu e cosa sono io...\n",
            " PREDICTED: — Ma , ma non ti prego , e io non posso dire ...\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from config import get_config, latest_weights_file_path\n",
        "from model import build_transformer\n",
        "from tokenizers import Tokenizer\n",
        "from datasets import load_dataset\n",
        "from dataset import BilingualDataset\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "def translate(sentence: str):\n",
        "    # Define the device, tokenizers, and model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "    config = get_config()\n",
        "    tokenizer_src = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_src']))))\n",
        "    tokenizer_tgt = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_tgt']))))\n",
        "    model = build_transformer(tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size(), config[\"seq_len\"], config['seq_len'], d_model=config['d_model']).to(device)\n",
        "\n",
        "    # Load the pretrained weights\n",
        "    model_filename = latest_weights_file_path(config)\n",
        "    state = torch.load(model_filename)\n",
        "    model.load_state_dict(state['model_state_dict'])\n",
        "\n",
        "    # if the sentence is a number use it as an index to the test set\n",
        "    label = \"\"\n",
        "    if type(sentence) == int or sentence.isdigit():\n",
        "        id = int(sentence)\n",
        "        ds = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='all')\n",
        "        ds = BilingualDataset(ds, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "        sentence = ds[id]['src_text']\n",
        "        label = ds[id][\"tgt_text\"]\n",
        "    seq_len = config['seq_len']\n",
        "\n",
        "    # translate the sentence\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Precompute the encoder output and reuse it for every generation step\n",
        "        source = tokenizer_src.encode(sentence)\n",
        "        source = torch.cat([\n",
        "            torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64),\n",
        "            torch.tensor(source.ids, dtype=torch.int64),\n",
        "            torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64),\n",
        "            torch.tensor([tokenizer_src.token_to_id('[PAD]')] * (seq_len - len(source.ids) - 2), dtype=torch.int64)\n",
        "        ], dim=0).to(device)\n",
        "        source_mask = (source != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n",
        "        encoder_output = model.encode(source, source_mask)\n",
        "\n",
        "        # Initialize the decoder input with the sos token\n",
        "        decoder_input = torch.empty(1, 1).fill_(tokenizer_tgt.token_to_id('[SOS]')).type_as(source).to(device)\n",
        "\n",
        "        # Print the source sentence and target start prompt\n",
        "        if label != \"\": print(f\"{f'ID: ':>12}{id}\")\n",
        "        print(f\"{f'SOURCE: ':>12}{sentence}\")\n",
        "        if label != \"\": print(f\"{f'TARGET: ':>12}{label}\")\n",
        "        print(f\"{f'PREDICTED: ':>12}\", end='')\n",
        "\n",
        "        # Generate the translation word by word\n",
        "        while decoder_input.size(1) < seq_len:\n",
        "            # build mask for target and calculate output\n",
        "            decoder_mask = torch.triu(torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1).type(torch.int).type_as(source_mask).to(device)\n",
        "            out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "            # project next token\n",
        "            prob = model.project(out[:, -1])\n",
        "            _, next_word = torch.max(prob, dim=1)\n",
        "            decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "            # print the translated word\n",
        "            print(f\"{tokenizer_tgt.decode([next_word.item()])}\", end=' ')\n",
        "\n",
        "            # break if we predict the end of sentence token\n",
        "            if next_word == tokenizer_tgt.token_to_id('[EOS]'):\n",
        "                break\n",
        "\n",
        "    # convert ids to tokens\n",
        "    return tokenizer_tgt.decode(decoder_input[0].tolist())\n",
        "\n",
        "#read sentence from argument\n",
        "translate(sys.argv[1] if len(sys.argv) > 1 else \"I am not a very good a student.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "yNyz_eellPIc",
        "outputId": "4badb289-88a8-41c4-cb76-0b8324f95e1e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "    SOURCE: -f\n",
            " PREDICTED: I  "
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = translate(\"Why do I need to translate this?\")\n",
        "t"
      ],
      "metadata": {
        "id": "jR9qrSxoWNH9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b43d0829-ecb2-4666-c594-6eb4406ea222"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "    SOURCE: Why do I need to translate this?\n",
            " PREDICTED: Perché io sono .   "
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Perché io sono .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = translate(34)"
      ],
      "metadata": {
        "id": "U0mKLdihmqbv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a117a7a-9d90-4e4e-d180-8163371d502c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "        ID: 34\n",
            "    SOURCE: And I came out immediately, for I trembled at the idea of being dragged forth by the said Jack.\n",
            "    TARGET: Uscii subito, perché mi sgomentavo al pensiero di esser condotta fuori dal mio nascondiglio da John.\n",
            " PREDICTED: E io mi                    "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}